{"cells":[{"cell_type":"markdown","id":"_kbUSCb_Egcu","metadata":{"id":"_kbUSCb_Egcu"},"source":["\n","HuggingFace Transformers를 활용한 토큰 분류 모델 학습"]},{"cell_type":"markdown","id":"of2izij8Eqe0","metadata":{"id":"of2izij8Eqe0"},"source":["본 노트북에서는 `klue/roberta-base` 모델을 **KLUE** 내 **NLI** 데이터셋을 활용하여 모델을 훈련하는 예제를 다루게 됩니다.\n","\n","\n","학습 과정 이후에는 간단한 예제 코드를 통해 모델이 어떻게 활용되는지도 함께 알아보도록 할 것입니다.\n","\n","모든 소스 코드는 [`huggingface-tutorial`](https://huggingface.co/course/chapter7/2)를 참고하였습니다. \n","\n","먼저, 노트북을 실행하는데 필요한 라이브러리를 설치합니다. 모델 훈련을 위해서는 `transformers`가, 학습 데이터셋 로드를 위해서는 `datasets` 라이브러리의 설치가 필요합니다. 그 외 모델 성능 검증을 위해 `scipy`, `scikit-learn`을 추가로 설치해주도록 합니다."]},{"cell_type":"code","source":["#https://towardsdatascience.com/how-to-create-and-train-a-multi-task-transformer-model-18c54a14624\n","#https://medium.com/@shahrukhx01/multi-task-learning-with-transformers-part-1-multi-prediction-heads-b7001cf014bf"],"metadata":{"id":"MGaqyMD52aGa"},"id":"MGaqyMD52aGa","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install  evaluate \n","!pip install accelerate\n","# To run the training on TPU, you will need to uncomment the following line:\n","# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n","!apt install git-lfs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-FiiDKPiD_Et","executionInfo":{"status":"ok","timestamp":1679737719992,"user_tz":-540,"elapsed":24241,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}},"outputId":"e20452ab-316a-440e-9e6a-65f4999637bb"},"id":"-FiiDKPiD_Et","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting evaluate\n","  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess\n","  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets>=2.0.0\n","  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.65.0)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.27.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.22.4)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2023.3.0)\n","Collecting dill\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.4.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (23.0)\n","Collecting huggingface-hub>=0.7.0\n","  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.10.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.7.1)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n","Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets, evaluate\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.10.1 dill-0.3.6 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.13.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting accelerate\n","  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.13.1+cu116)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.4)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.22.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (4.5.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.18.0\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","git-lfs is already the newest version (2.9.2-1).\n","0 upgraded, 0 newly installed, 0 to remove and 23 not upgraded.\n"]}]},{"cell_type":"code","execution_count":2,"id":"8e59dc52","metadata":{"id":"8e59dc52","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679737738797,"user_tz":-540,"elapsed":18811,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}},"outputId":"cd337f3a-e831-40c4-82a3-88950f0828a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.10.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (1.10.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.1.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: tokenizers, transformers\n","Successfully installed tokenizers-0.13.2 transformers-4.27.3\n"]}],"source":["!pip install -U transformers datasets scipy scikit-learn"]},{"cell_type":"code","source":["#from huggingface_hub import notebook_login\n","\n","#notebook_login()"],"metadata":{"id":"XLLtb_LzJNgW"},"id":"XLLtb_LzJNgW","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2O8kHxYWJOZE"},"id":"2O8kHxYWJOZE","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"8wSLfX-jaIqV","metadata":{"id":"8wSLfX-jaIqV"},"source":["## 문장 분류 모델 학습"]},{"cell_type":"markdown","id":"92e868b7-3cf7-4977-a6e7-ebbb99ba298e","metadata":{"id":"92e868b7-3cf7-4977-a6e7-ebbb99ba298e"},"source":["노트북을 실행하는데 필요한 라이브러리들을 모두 임포트합니다."]},{"cell_type":"code","execution_count":11,"id":"243b461f","metadata":{"id":"243b461f","executionInfo":{"status":"ok","timestamp":1679737871437,"user_tz":-540,"elapsed":365,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}}},"outputs":[],"source":["import random\n","import logging\n","from IPython.display import display, HTML\n","\n","import numpy as np\n","import pandas as pd\n","import datasets\n","from datasets import load_dataset, load_metric, ClassLabel, Sequence\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoModel, PreTrainedTokenizer\n"]},{"cell_type":"markdown","id":"59b1e5ae-471f-461f-850f-4241a6037471","metadata":{"id":"59b1e5ae-471f-461f-850f-4241a6037471"},"source":["학습에 필요한 정보를 변수로 기록합니다.\n","\n","본 노트북에서는 `klue-roberta-base` 모델을 활용하지만, https://huggingface.co/klue 페이지에서 더 다양한 사전학습 언어 모델을 확인하실 수 있습니다.\n","\n","학습 태스크로는 `nli`를, 배치 사이즈로는 32를 지정하겠습니다."]},{"cell_type":"code","execution_count":4,"id":"6ecaacbe","metadata":{"id":"6ecaacbe","executionInfo":{"status":"ok","timestamp":1679737790208,"user_tz":-540,"elapsed":387,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}}},"outputs":[],"source":["model_checkpoint = \"klue/bert-base\"\n","batch_size = 64\n","task = \"dp\""]},{"cell_type":"markdown","id":"816b5893-d03e-4c6e-8536-e246b1def987","metadata":{"id":"816b5893-d03e-4c6e-8536-e246b1def987"},"source":["이제 HuggingFace `datasets` 라이브러리에 등록된 KLUE 데이터셋 중, NLI 데이터를 내려받습니다."]},{"cell_type":"code","execution_count":null,"id":"f8feeba0","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["95b7970d076b408dbf4f1c22ae112256","9bc908af4a9c4376af8f8552a6f88c3c","a09ca482422c49e4a873bf1e2effddd1","9d0afd3d43494868b0f4818d8bd63463","34d9dd10d0dc4c828b34405715680bd3","16108b92b4574e88a8f5407563390269","c8f9556dd395494286beecae03d660a9","446d8b86da824c5c823041b48ed88096","6d76b79a2bd44468a0e20adcd005fa58","45ceaf6f3cb44103af376c5bb1c576d1","1c7ee48824cc41d1b3c8cf3f296da2c1"]},"id":"f8feeba0","outputId":"2a7ac7f5-36ac-4c94-be2d-e3fe1f3e3b77","executionInfo":{"status":"ok","timestamp":1679665431375,"user_tz":-540,"elapsed":1791,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Found cached dataset klue (/root/.cache/huggingface/datasets/klue/dp/1.0.0/e0fc3bc3de3eb03be2c92d72fd04a60ecc71903f821619cb28ca0e1e29e4233e)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95b7970d076b408dbf4f1c22ae112256"}},"metadata":{}}],"source":["#['ynat', 'sts', 'nli', 'ner', 're', 'dp', 'mrc', 'wos']\n","raw_datasets = load_dataset(\"klue\", task)"]},{"cell_type":"markdown","id":"f90b06e4-3795-435a-aec8-701da9bd5f04","metadata":{"id":"f90b06e4-3795-435a-aec8-701da9bd5f04"},"source":["다운로드 혹은 로드 후 얻어진 `datasets` 객체를 살펴보면, 훈련 데이터와 검증 데이터가 포함되어 있는 것을 확인할 수 있습니다."]},{"cell_type":"code","source":["logger = logging.getLogger()\n","logger.setLevel(logging.INFO)\n","logging.basicConfig()\n"],"metadata":{"id":"SQq8Po1WcgTc","executionInfo":{"status":"ok","timestamp":1679737795474,"user_tz":-540,"elapsed":2,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}}},"id":"SQq8Po1WcgTc","execution_count":5,"outputs":[]},{"cell_type":"code","source":["raw_datasets"],"metadata":{"id":"1rb8ud37aWpy","executionInfo":{"status":"ok","timestamp":1679665440340,"user_tz":-540,"elapsed":1453,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"39a3bc81-1e93-421d-d38f-e0c887582ac4"},"id":"1rb8ud37aWpy","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['sentence', 'index', 'word_form', 'lemma', 'pos', 'head', 'deprel'],\n","        num_rows: 10000\n","    })\n","    validation: Dataset({\n","        features: ['sentence', 'index', 'word_form', 'lemma', 'pos', 'head', 'deprel'],\n","        num_rows: 2000\n","    })\n","})"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","execution_count":null,"id":"QzlmWQ2DrZ7P","metadata":{"id":"QzlmWQ2DrZ7P"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"5Qb0lNG0aXFX"},"id":"5Qb0lNG0aXFX","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"f39_pKKnarD3","metadata":{"id":"f39_pKKnarD3"},"source":["각 예시 데이터는 아래와 같이 두 개의 문장과 두 문장의 추론 관계를 라벨로 지니고 있습니다."]},{"cell_type":"code","source":[],"metadata":{"id":"HTihjdNSHDU_"},"id":"HTihjdNSHDU_","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tdZHPtgpHDf4"},"id":"tdZHPtgpHDf4","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":15,"id":"e46bf44d","metadata":{"id":"e46bf44d","executionInfo":{"status":"ok","timestamp":1679737907227,"user_tz":-540,"elapsed":910,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}}},"outputs":[],"source":["\n","def get_dep_labels() -> List[str]:\n","    \"\"\"\n","    label for dependency relations format:\n","    {structure}_(optional){function}\n","    \"\"\"\n","    dep_labels = [\n","        \"NP\",\n","        \"NP_AJT\",\n","        \"VP\",\n","        \"NP_SBJ\",\n","        \"VP_MOD\",\n","        \"NP_OBJ\",\n","        \"AP\",\n","        \"NP_CNJ\",\n","        \"NP_MOD\",\n","        \"VNP\",\n","        \"DP\",\n","        \"VP_AJT\",\n","        \"VNP_MOD\",\n","        \"NP_CMP\",\n","        \"VP_SBJ\",\n","        \"VP_CMP\",\n","        \"VP_OBJ\",\n","        \"VNP_CMP\",\n","        \"AP_MOD\",\n","        \"X_AJT\",\n","        \"VP_CNJ\",\n","        \"VNP_AJT\",\n","        \"IP\",\n","        \"X\",\n","        \"X_SBJ\",\n","        \"VNP_OBJ\",\n","        \"VNP_SBJ\",\n","        \"X_OBJ\",\n","        \"AP_AJT\",\n","        \"L\",\n","        \"X_MOD\",\n","        \"X_CNJ\",\n","        \"VNP_CNJ\",\n","        \"X_CMP\",\n","        \"AP_CMP\",\n","        \"AP_SBJ\",\n","        \"R\",\n","        \"NP_SVJ\",\n","    ]\n","    return dep_labels\n","\n","\n","def get_pos_labels() -> List[str]:\n","    \"\"\"label for part-of-speech tags\"\"\"\n","\n","    return [\n","        \"NNG\",\n","        \"NNP\",\n","        \"NNB\",\n","        \"NP\",\n","        \"NR\",\n","        \"VV\",\n","        \"VA\",\n","        \"VX\",\n","        \"VCP\",\n","        \"VCN\",\n","        \"MMA\",\n","        \"MMD\",\n","        \"MMN\",\n","        \"MAG\",\n","        \"MAJ\",\n","        \"JC\",\n","        \"IC\",\n","        \"JKS\",\n","        \"JKC\",\n","        \"JKG\",\n","        \"JKO\",\n","        \"JKB\",\n","        \"JKV\",\n","        \"JKQ\",\n","        \"JX\",\n","        \"EP\",\n","        \"EF\",\n","        \"EC\",\n","        \"ETN\",\n","        \"ETM\",\n","        \"XPN\",\n","        \"XSN\",\n","        \"XSV\",\n","        \"XSA\",\n","        \"XR\",\n","        \"SF\",\n","        \"SP\",\n","        \"SS\",\n","        \"SE\",\n","        \"SO\",\n","        \"SL\",\n","        \"SH\",\n","        \"SW\",\n","        \"SN\",\n","        \"NA\",\n","    ]"]},{"cell_type":"markdown","id":"pc1BXYLYaupD","metadata":{"id":"pc1BXYLYaupD"},"source":["데이터셋을 전반적으로 살펴보기 위한 시각화 함수를 다음과 같이 정의합니다."]},{"cell_type":"code","execution_count":null,"id":"8110a3ee","metadata":{"id":"8110a3ee"},"outputs":[],"source":["def show_random_elements(dataset, num_examples=10):\n","    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n","\n","    picks = []\n","    \n","    for _ in range(num_examples):\n","        pick = random.randint(0, len(dataset)-1)\n","\n","        # 이미 등록된 예제가 뽑힌 경우, 다시 추출\n","        while pick in picks:\n","            pick = random.randint(0, len(dataset)-1)\n","\n","        picks.append(pick)\n","\n","    # 임의로 추출된 인덱스들로 구성된 데이터 프레임 선언\n","    df = pd.DataFrame(dataset[picks])\n","\n","    for column, typ in dataset.features.items():\n","        # 라벨 클래스를 스트링으로 변환\n","        if isinstance(typ, ClassLabel):\n","            df[column] = df[column].transform(lambda i: typ.names[i])\n","\n","    display(HTML(df.to_html()))"]},{"cell_type":"markdown","id":"u1Ia2wGh1xaF","metadata":{"id":"u1Ia2wGh1xaF"},"source":["앞서 정의한 함수를 활용해 훈련 데이터를 살펴보도록 합시다.\n","\n","이처럼 데이터를 살펴보는 것의 장점으로는 각 라벨에 어떠한 문장들이 해당하는지에 대한 감을 익힐 수 있다는데에 있습니다.\n"]},{"cell_type":"code","execution_count":null,"id":"8c7888d1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":658},"id":"8c7888d1","outputId":"8be407d1-86b8-4ef5-efc6-fc8524a773a3","scrolled":true,"executionInfo":{"status":"ok","timestamp":1679633888968,"user_tz":-540,"elapsed":5,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>index</th>\n","      <th>word_form</th>\n","      <th>lemma</th>\n","      <th>pos</th>\n","      <th>head</th>\n","      <th>deprel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>현재 노조는 외환위기 극복을 위해 61세에서 58세로 단축된 정년을 공무원의 정년과 연동해 다시 연장하기로 단체협약을 4차례 맺었지만 이행되지 않고 있다고 주장하고 있다.</td>\n","      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]</td>\n","      <td>[현재, 노조는, 외환위기, 극복을, 위해, 61세에서, 58세로, 단축된, 정년을, 공무원의, 정년과, 연동해, 다시, 연장하기로, 단체협약을, 4차례, 맺었지만, 이행되지, 않고, 있다고, 주장하고, 있다.]</td>\n","      <td>[현재, 노조 는, 외환 위기, 극복 을, 위하 여, 61 세 에서, 58 세 로, 단축 되 ㄴ, 정년 을, 공무원 의, 정년 과, 연동 하 여, 다시, 연장 하 기 로, 단체 협약 을, 4 차례, 맺 었 지만, 이행 되 지, 않 고, 있 다고, 주장 하 고, 있 다 .]</td>\n","      <td>[MAG, NNG+JX, NNG+NNG, NNG+JKO, VV+EC, SN+NNB+JKB, SN+NNB+JKB, NNG+XSV+ETM, NNG+JKO, NNG+JKG, NNG+JKB, NNG+XSV+EC, MAG, NNG+XSV+ETN+JKB, NNG+NNG+JKO, SN+NNG, VV+EP+EC, NNG+XSV+EC, VX+EC, VX+EC, NNG+XSV+EC, VX+EF+SF]</td>\n","      <td>[21, 21, 4, 5, 12, 7, 8, 9, 12, 11, 12, 14, 14, 17, 17, 17, 18, 19, 20, 21, 22, 0]</td>\n","      <td>[AP, NP_SBJ, NP, NP_OBJ, VP, NP_AJT, NP_AJT, VP_MOD, NP_OBJ, NP_MOD, NP_AJT, VP, AP, VP_AJT, NP_OBJ, NP_AJT, VP, VP, VP, VP_CMP, VP, VP]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>영어가 익숙하지 않은 분들은 아예 이용이 어려울 수 있는 정도예요.</td>\n","      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</td>\n","      <td>[영어가, 익숙하지, 않은, 분들은, 아예, 이용이, 어려울, 수, 있는, 정도예요.]</td>\n","      <td>[영어 가, 익숙하 지, 않 은, 분 들 은, 아예, 이용 이, 어렵 ㄹ, 수, 있 는, 정도 이 에요 .]</td>\n","      <td>[NNG+JKS, VA+EC, VX+ETM, NNB+XSN+JX, MAG, NNG+JKS, VA+ETM, NNB, VV+ETM, NNG+VCP+EF+SF]</td>\n","      <td>[2, 3, 4, 7, 7, 7, 8, 9, 10, 0]</td>\n","      <td>[NP_SBJ, VP, VP_MOD, NP_SBJ, AP, NP_SBJ, VP_MOD, NP_SBJ, VP_MOD, VNP]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>야외활동이 적은 1∼4월 월평균 20마리 정도의 동물이 버려진 것과 비교하면 2∼3배 늘어난 수치다.</td>\n","      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]</td>\n","      <td>[야외활동이, 적은, 1∼4월, 월평균, 20마리, 정도의, 동물이, 버려진, 것과, 비교하면, 2∼3배, 늘어난, 수치다.]</td>\n","      <td>[야외 활동 이, 적 은, 1 ∼ 4 월, 월 평균, 20 마리, 정도 의, 동물 이, 버리 어 지 ㄴ, 것 과, 비교 하 면, 2 ∼ 3 배, 늘 어 나 ㄴ, 수치 이 다 .]</td>\n","      <td>[NNG+NNG+JKS, VA+ETM, SN+SO+SN+NNB, NNG+NNG, SN+NNB, NNG+JKG, NNG+JKS, VV+EC+VX+ETM, NNB+JKB, NNG+XSV+EC, SN+SO+SN+NNG, VV+EC+VX+ETM, NNG+VCP+EF+SF]</td>\n","      <td>[2, 3, 8, 5, 6, 7, 8, 9, 10, 13, 12, 13, 0]</td>\n","      <td>[NP_SBJ, VP_MOD, NP_AJT, NP, NP, NP_MOD, NP_SBJ, VP_MOD, NP_AJT, VP, NP_SBJ, VP_MOD, VNP]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>아직 지진으로 인한 피해상황은 전해지지 않고 있다.</td>\n","      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n","      <td>[아직, 지진으로, 인한, 피해상황은, 전해지지, 않고, 있다.]</td>\n","      <td>[아직, 지진 으로, 인하 ㄴ, 피해 상황 은, 전하 여 지 지, 않 고, 있 다 .]</td>\n","      <td>[MAG, NNG+JKB, VV+ETM, NNG+NNG+JX, VV+EC+VX+EC, VX+EC, VX+EF+SF]</td>\n","      <td>[5, 3, 4, 5, 6, 7, 0]</td>\n","      <td>[AP, NP_AJT, VP_MOD, NP_SBJ, VP, VP, VP]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>공항공사 노동조합과 용산참사 유족 등이 한국공항공사 신임 사장으로 임명된 김석기 전 서울지방경찰청장의 출근을 저지하기 위해 사옥 앞에 서 있습니다.</td>\n","      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]</td>\n","      <td>[공항공사, 노동조합과, 용산참사, 유족, 등이, 한국공항공사, 신임, 사장으로, 임명된, 김석기, 전, 서울지방경찰청장의, 출근을, 저지하기, 위해, 사옥, 앞에, 서, 있습니다.]</td>\n","      <td>[공항공사, 노동 조합 과, 용산 참사, 유족, 등 이, 한국공항공사, 신임, 사장 으로, 임명 되 ㄴ, 김석기, 전, 서울지방경찰청장 의, 출근 을, 저지 하 기, 위하 여, 사옥, 앞 에, 서 어, 있 습니다 .]</td>\n","      <td>[NNP, NNG+NNG+JC, NNP+NNG, NNG, NNB+JKS, NNP, NNG, NNG+JKB, NNG+XSV+ETM, NNP, MMD, NNP+JKG, NNG+JKO, NNG+XSV+ETN, VV+EC, NNG, NNG+JKB, VV+EC, VX+EF+SF]</td>\n","      <td>[2, 4, 4, 5, 15, 8, 8, 9, 12, 12, 12, 13, 14, 15, 18, 17, 18, 19, 0]</td>\n","      <td>[NP, NP_CNJ, NP, NP, NP_SBJ, NP, NP, NP_AJT, VP_MOD, NP, DP, NP_MOD, NP_OBJ, VP_OBJ, VP, NP, NP_AJT, VP, VP]</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>LG전자는 스마트폰 광고 모델로 '체조요정' 손연재 선수를 선정했다고 16일 밝혔다.</td>\n","      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</td>\n","      <td>[LG전자는, 스마트폰, 광고, 모델로, '체조요정', 손연재, 선수를, 선정했다고, 16일, 밝혔다.]</td>\n","      <td>[LG 전자 는, 스마트 폰, 광고, 모델 로, ' 체조 요정 ', 손연재, 선수 를, 선정 하 였 다고, 16 일, 밝히 었 다 .]</td>\n","      <td>[SL+NNG+JX, NNG+NNG, NNG, NNG+JKB, SS+NNG+NNG+SS, NNP, NNG+JKO, NNG+XSV+EP+EC, SN+NNB, VV+EP+EF+SF]</td>\n","      <td>[10, 4, 4, 8, 7, 7, 8, 10, 10, 0]</td>\n","      <td>[NP_SBJ, NP, NP, NP_AJT, NP, NP, NP_OBJ, VP_AJT, NP_AJT, VP]</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>일단 런던에서 이 정도 가격에 이런 숙소를 쓸 수 있다는게 너무 좋았어요.</td>\n","      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]</td>\n","      <td>[일단, 런던에서, 이, 정도, 가격에, 이런, 숙소를, 쓸, 수, 있다는게, 너무, 좋았어요.]</td>\n","      <td>[일단, 런던 에서, 이, 정도, 가격 에, 이런, 숙소 를, 쓰 ㄹ, 수, 있 다는 것 이, 너무, 좋 았 어요 .]</td>\n","      <td>[MAG, NNP+JKB, MMD, NNG, NNG+JKB, MMD, NNG+JKO, VV+ETM, NNB, VA+ETM+NNB+JKS, MAG, VA+EP+EF+SF]</td>\n","      <td>[12, 8, 4, 5, 8, 7, 8, 9, 10, 12, 12, 0]</td>\n","      <td>[AP, NP_AJT, DP, NP, NP_AJT, DP, NP_OBJ, VP_MOD, NP_SBJ, NP_SBJ, AP, VP]</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>양측은 판문점 남측 지역 평화의 집에서 추석 계기 이산가족 상봉 행사 등을 논의하는 무박 2일의 적십자 실무접촉을 갖고 이런 내용이 포함된 2개항의 합의서를 채택했다.</td>\n","      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]</td>\n","      <td>[양측은, 판문점, 남측, 지역, 평화의, 집에서, 추석, 계기, 이산가족, 상봉, 행사, 등을, 논의하는, 무박, 2일의, 적십자, 실무접촉을, 갖고, 이런, 내용이, 포함된, 2개항의, 합의서를, 채택했다.]</td>\n","      <td>[양측 은, 판문점, 남측, 지역, 평화 의, 집 에서, 추석, 계기, 이산가족, 상봉, 행사, 등 을, 논의 하 는, 무 박, 2 일 의, 적십자, 실무 접촉 을, 갖 고, 이런, 내용 이, 포함 되 ㄴ, 2 개 항 의, 합의서 를, 채택 하 였 다 .]</td>\n","      <td>[NNG+JX, NNP, NNG, NNG, NNG+JKG, NNG+JKB, NNG, NNG, NNG, NNG, NNG, NNB+JKO, NNG+XSV+ETM, XPN+NNG, SN+NNB+JKG, NNG, NNG+NNG+JKO, VV+EC, MMD, NNG+JKS, NNG+XSV+ETM, SN+NNB+NNG+JKG, NNG+JKO, NNG+XSV+EP+EF+SF]</td>\n","      <td>[18, 3, 4, 6, 6, 18, 8, 11, 10, 11, 12, 13, 17, 15, 17, 17, 18, 24, 20, 21, 23, 23, 24, 0]</td>\n","      <td>[NP_SBJ, NP, NP, NP, NP_MOD, NP_AJT, NP, NP, NP, NP, NP, NP_OBJ, VP_MOD, NP, NP_MOD, NP, NP_OBJ, VP, DP, NP_SBJ, VP_MOD, NP_MOD, NP_OBJ, VP]</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>위치, 물 수압, 호스트 모두모두 아주 좋습니다.</td>\n","      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n","      <td>[위치,, 물, 수압,, 호스트, 모두모두, 아주, 좋습니다.]</td>\n","      <td>[위치 ,, 물, 수압 ,, 호스트, 모두 모두, 아주, 좋 습니다 .]</td>\n","      <td>[NNG+SP, NNG, NNG+SP, NNG, MAG+MAG, MAG, VA+EF+SF]</td>\n","      <td>[4, 3, 4, 7, 7, 7, 0]</td>\n","      <td>[NP_CNJ, NP, NP_CNJ, NP_SBJ, AP, AP, VP]</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>가까운 예로 지난달 현 정권 실세비리를 폭로한 이국철 SLS그룹 회장의 항소심을 심리하던 서울고법은 구속만기일에 임박하자 보석 심문을 통해 직권으로 풀어준 적이 있다.</td>\n","      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]</td>\n","      <td>[가까운, 예로, 지난달, 현, 정권, 실세비리를, 폭로한, 이국철, SLS그룹, 회장의, 항소심을, 심리하던, 서울고법은, 구속만기일에, 임박하자, 보석, 심문을, 통해, 직권으로, 풀어준, 적이, 있다.]</td>\n","      <td>[가깝 ㄴ, 예 로, 지나 ㄴ 달, 현, 정권, 실세 비리 를, 폭로 하 ㄴ, 이국철, SLS 그룹, 회장 의, 항소심 을, 심리 하 던, 서울고법 은, 구속 만기일 에, 임박 하 자, 보석, 심문 을, 통하 여, 직권 으로, 풀 어 주 ㄴ, 적 이, 있 다 .]</td>\n","      <td>[VA+ETM, NNG+JKB, VV+ETM+NNG, MMD, NNG, NNG+NNG+JKO, NNG+XSV+ETM, NNP, SL+NNG, NNG+JKG, NNG+JKO, NNG+XSV+ETM, NNP+JX, NNG+NNG+JKB, NNG+XSV+EC, NNG, NNG+JKO, VV+EC, NNG+JKB, VV+EC+VX+ETM, NNB+JKS, VV+EF+SF]</td>\n","      <td>[2, 15, 7, 5, 6, 7, 10, 9, 10, 11, 12, 13, 15, 15, 18, 17, 18, 20, 20, 21, 22, 0]</td>\n","      <td>[VP_MOD, NP_AJT, NP_AJT, DP, NP, NP_OBJ, VP_MOD, NP, NP, NP_MOD, NP_OBJ, VP_MOD, NP_SBJ, NP_AJT, VP, NP, NP_OBJ, VP, NP_AJT, VP_MOD, NP_SBJ, VP]</td>\n","    </tr>\n","  </tbody>\n","</table>"]},"metadata":{}}],"source":["show_random_elements(datasets[\"train\"])"]},{"cell_type":"markdown","id":"zakMS4Y61sLm","metadata":{"id":"zakMS4Y61sLm"},"source":["훈련 과정 중 모델의 성능을 파악하기 위한 메트릭을 설정합니다.\n","\n","`datasets` 라이브러리에는 이미 구현된 메트릭을 사용할 수 있는 `load_metric` 함수가 있습니다.\n"]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True) "],"metadata":{"id":"5jRyyn1CBBDm","colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["1a3feeb86d234d57a068ff7576544a62","ad9f4a7c2e1e414e81e19f96f03f0cbf","1034f18cc80b442890021f8eeb3ad1be","874bffba26514b0a8dc855b1a14c19c0","8874354c0bfd41eea1c7d0da1b5f01d8","4e9acd67f87044d9aee23a4ed37f03b6","a8bc640b7d93476fb0ff53f7272b6057","4ad548dc5c5a4ef8b434fff1075e847a","2a93346a50464032b0a4943e8df407b9","30243c264c904dd58d6389d11635d75b","3c3e72678af64df09a29e3b131f3681e","4ef91e73cf8748cab38e6ebfd02145d3","691efb7e903743a6839f9f26bccb37b9","2a69d6aac70b45b88112a75012286fe7","4121f786ee4a4084bb009587141147d4","c70a307135374298ac167bc4948a6d91","75d7a131e6ac40b7b03898441e66472b","bcfc09c414de496f82ac69d9f10e1490","c12b63599bab4676a22b584801d1aae2","234ef66bd73b45e389eb37a85bf1dead","d26f0a0ce2c04f408657ac34da6d783e","816c42483e7e45dbb4ec944fa06661e9","52e3045105ca402da4ea94f522d769fd","195395425ebe425bad365d5eec403fa6","a9f7ad593e214072a54e8735fb2febf6","20d393936ec843d3aa78e9168fe4a09f","f0aee027338d44cbabb9ad9cd9b4898b","0eecb986aa654487922b6d982be7374a","3f5714a646434a8694acf2c023ed1c7f","8c89f70b2de348c8b89d5e041f325a18","5ffc4d01b9a34d5a97378960226f189e","fac3e74578764d96bcf93a0c6bf7439b","b84866ff9ee04c8abed8e9a8c9cd9887","7a26a17c491d45a29b0118de8df55be9","e03183718003498c8e680bab85439104","7c4f060286e1494987f700bef3b4e1f7","ad3b3e441c854660b62324a5fb942c4c","e21510ac88c54f27813ce1a1ee27a55d","35aa963ce7fc460bae814e31a1cf0f21","9d24077ae03346918dfa4073d4688644","c200cfb8a2cf4beeb47be9f45b7739dc","93f149abba2545bd9c088fbe5242a5ea","3fd4ddd1e215467c90bb275802ee839e","b52e4ad67e01402ab3ddff1e9695eb36","8899ccfab14f4d9fbf50a114a4a74fa6","a4de7f7528a54452a6680e9778aba182","1d38920268f64136bd7c14f0668f04f9","c5f93a66e9af43bd8c8286c121348c84","95de76ea787b4607ba5168d2fa56261d","56044980c60e41d6ac8e49e011f958ca","6bb78bc983d74713822792c91fed4c4e","06f8b07dc7f54659857041c44c20b069","25ad51c1a8c3438ca0e27ca31e34007d","22029c9ab46c421981fa6c8ff78bf1d4","243b1784f29c43f6b4962f996c3950cb"]},"executionInfo":{"status":"ok","timestamp":1679737807851,"user_tz":-540,"elapsed":5420,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}},"outputId":"b0f2a117-0bba-40c4-8366-ad724ad83daf"},"id":"5jRyyn1CBBDm","execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a3feeb86d234d57a068ff7576544a62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/425 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ef91e73cf8748cab38e6ebfd02145d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52e3045105ca402da4ea94f522d769fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/495k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a26a17c491d45a29b0118de8df55be9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8899ccfab14f4d9fbf50a114a4a74fa6"}},"metadata":{}}]},{"cell_type":"code","source":["import argparse\n","import logging\n","import os\n","from typing import Any, List, Optional, Tuple\n"],"metadata":{"id":"4vX--P-5CD-y","executionInfo":{"status":"ok","timestamp":1679737810216,"user_tz":-540,"elapsed":354,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}}},"id":"4vX--P-5CD-y","execution_count":7,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"osBh9cJwIpvC","executionInfo":{"status":"ok","timestamp":1679737810722,"user_tz":-540,"elapsed":3,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}}},"id":"osBh9cJwIpvC","execution_count":7,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ChCtIdK20UzQ"},"id":"ChCtIdK20UzQ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["pos_labels = get_pos_labels()\n","dep_labels = get_dep_labels()"],"metadata":{"id":"Vt0nkzn71QDN","executionInfo":{"status":"ok","timestamp":1679737916412,"user_tz":-540,"elapsed":347,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}}},"id":"Vt0nkzn71QDN","execution_count":16,"outputs":[]},{"cell_type":"code","source":["import torch"],"metadata":{"id":"69XEJefC5S-m","executionInfo":{"status":"ok","timestamp":1679737814448,"user_tz":-540,"elapsed":2,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}}},"id":"69XEJefC5S-m","execution_count":8,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, TensorDataset\n"],"metadata":{"id":"4rL7ut5lDuh0","executionInfo":{"status":"ok","timestamp":1679737814839,"user_tz":-540,"elapsed":4,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}}},"id":"4rL7ut5lDuh0","execution_count":9,"outputs":[]},{"cell_type":"code","source":["\n","class KlueDPInputExample:\n","    \"\"\"A single training/test example for Dependency Parsing in .conllu format\n","    Args:\n","        guid : Unique id for the example\n","        text : string. the original form of sentence\n","        token_id : token id\n","        token : 어절\n","        pos : POS tag(s)\n","        head : dependency head\n","        dep : dependency relation\n","    \"\"\"\n","\n","    def __init__(\n","        self, guid: str, text: str, sent_id: int, token_id: int, token: str, pos: str, head: str, dep: str\n","    ) -> None:\n","        self.guid = guid\n","        self.text = text\n","        self.sent_id = sent_id\n","        self.token_id = token_id\n","        self.token = token\n","        self.pos = pos\n","        self.head = head\n","        self.dep = dep\n","\n","\n","class KlueDPInputFeatures:\n","    \"\"\"A single set of features of data. Property names are the same names as the corresponding inputs to a model.\n","    Args:\n","        input_ids: Indices of input sequence tokens in the vocabulary.\n","        attention_mask: Mask to avoid performing attention on padding token indices.\n","            Mask values selected in ``[0, 1]``: Usually ``1`` for tokens that are NOT MASKED, ``0`` for MASKED (padded)\n","            tokens.\n","        bpe_head_mask : Mask to mark the head token of bpe in aejeol\n","        head_ids : head ids for each aejeols on head token index\n","        dep_ids : dependecy relations for each aejeols on head token index\n","        pos_ids : pos tag for each aejeols on head token index\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        guid: str,\n","        ids: List[int],\n","        mask: List[int],\n","        bpe_head_mask: List[int],\n","        bpe_tail_mask: List[int],\n","        head_ids: List[int],\n","        dep_ids: List[int],\n","        pos_ids: List[int],\n","    ) -> None:\n","        self.guid = guid\n","        self.input_ids = ids\n","        self.attention_mask = mask\n","        self.bpe_head_mask = bpe_head_mask\n","        self.bpe_tail_mask = bpe_tail_mask\n","        self.head_ids = head_ids\n","        self.dep_ids = dep_ids\n","        self.pos_ids = pos_ids\n","\n","\n","class KlueDPProcessor:\n","\n","    origin_train_file_name = \"klue-dp-v1.1_train.tsv\"\n","    origin_dev_file_name = \"klue-dp-v1.1_dev.tsv\"\n","    origin_test_file_name = \"klue-dp-v1.1_test.tsv\"\n","\n","\n","    def __init__(self, max_seq_length: int, tokenizer: PreTrainedTokenizer) -> None:\n","        self.tokenizer = tokenizer\n","        self.max_seq_length = max_seq_length\n","\n","    def _create_examples(self, file_path: str, dataset_type: str) -> List[KlueDPInputExample]:\n","        sent_id = -1\n","        examples = []\n","        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","            for line in f:\n","                line = line.strip()\n","                if line == \"\" or line == \"\\n\" or line == \"\\t\":\n","                    continue\n","                if line.startswith(\"#\"):\n","                    parsed = line.strip().split(\"\\t\")\n","                    if len(parsed) != 2:  # metadata line about dataset\n","                        continue\n","                    else:\n","                        sent_id += 1\n","                        text = parsed[1].strip()\n","                        guid = parsed[0].replace(\"##\", \"\").strip()\n","                else:\n","                    token_list = [token.replace(\"\\n\", \"\") for token in line.split(\"\\t\")] + [\"-\", \"-\"]\n","                    examples.append(\n","                        KlueDPInputExample(\n","                            guid=guid,\n","                            text=text,\n","                            sent_id=sent_id,\n","                            token_id=int(token_list[0]),\n","                            token=token_list[1],\n","                            pos=token_list[3],\n","                            head=token_list[4],\n","                            dep=token_list[5],\n","                        )\n","                    )\n","        return examples\n","\n","    def convert_examples_to_features(\n","        self,\n","        examples: List[KlueDPInputExample],\n","        tokenizer: PreTrainedTokenizer,\n","        max_length: int,\n","        pos_label_list: List[str],\n","        dep_label_list: List[str],\n","    ) -> List[KlueDPInputFeatures]:\n","\n","        pos_label_map = {label: i for i, label in enumerate(pos_label_list)}\n","        dep_label_map = {label: i for i, label in enumerate(dep_label_list)}\n","\n","        SENT_ID = 0\n","\n","        token_list: List[str] = []\n","        pos_list: List[str] = []\n","        head_list: List[int] = []\n","        dep_list: List[str] = []\n","\n","        features = []\n","        for example in examples:\n","            if SENT_ID != example.sent_id:\n","                SENT_ID = example.sent_id\n","                encoded = tokenizer.encode_plus(\n","                    \" \".join(token_list),\n","                    None,\n","                    add_special_tokens=True,\n","                    max_length=max_length,\n","                    truncation=True,\n","                    padding=\"max_length\",\n","                )\n","\n","                ids, mask = encoded[\"input_ids\"], encoded[\"attention_mask\"]\n","\n","                bpe_head_mask = [0]\n","                bpe_tail_mask = [0]\n","                head_ids = [-100]\n","                dep_ids = [-100]\n","                pos_ids = [-100]  # --> CLS token\n","\n","                for token, head, dep, pos in zip(token_list, head_list, dep_list, pos_list):\n","                    bpe_len = len(tokenizer.tokenize(token))\n","                    head_token_mask = [1] + [0] * (bpe_len - 1)\n","                    tail_token_mask = [0] * (bpe_len - 1) + [1]\n","                    bpe_head_mask.extend(head_token_mask)\n","                    bpe_tail_mask.extend(tail_token_mask)\n","\n","                    head_mask = [head] + [-100] * (bpe_len - 1)\n","                    head_ids.extend(head_mask)\n","                    dep_mask = [dep_label_map[dep]] + [-100] * (bpe_len - 1)\n","                    dep_ids.extend(dep_mask)\n","                    pos_mask = [pos_label_map[pos]] + [-100] * (bpe_len - 1)\n","                    pos_ids.extend(pos_mask)\n","\n","                bpe_head_mask.append(0)\n","                bpe_tail_mask.append(0)\n","                head_ids.append(-100)\n","                dep_ids.append(-100)\n","                pos_ids.append(-100)  # END token\n","                if len(bpe_head_mask) > max_length:\n","                    bpe_head_mask = bpe_head_mask[:max_length]\n","                    bpe_tail_mask = bpe_tail_mask[:max_length]\n","                    head_ids = head_ids[:max_length]\n","                    dep_ids = dep_ids[:max_length]\n","                    pos_ids = pos_ids[:max_length]\n","\n","                else:\n","                    bpe_head_mask.extend([0] * (max_length - len(bpe_head_mask)))  # padding by max_len\n","                    bpe_tail_mask.extend([0] * (max_length - len(bpe_tail_mask)))  # padding by max_len\n","                    head_ids.extend([-100] * (max_length - len(head_ids)))  # padding by max_len\n","                    dep_ids.extend([-100] * (max_length - len(dep_ids)))  # padding by max_len\n","                    pos_ids.extend([-100] * (max_length - len(pos_ids)))\n","\n","                feature = KlueDPInputFeatures(\n","                    guid=example.guid,\n","                    ids=ids,\n","                    mask=mask,\n","                    bpe_head_mask=bpe_head_mask,\n","                    bpe_tail_mask=bpe_tail_mask,\n","                    head_ids=head_ids,\n","                    dep_ids=dep_ids,\n","                    pos_ids=pos_ids,\n","                )\n","                features.append(feature)\n","\n","                token_list = []\n","                pos_list = []\n","                head_list = []\n","                dep_list = []\n","\n","            token_list.append(example.token)\n","            pos_list.append(example.pos.split(\"+\")[-1])  # 맨 뒤 pos정보만 사용\n","            head_list.append(int(example.head))\n","            dep_list.append(example.dep)\n","\n","        encoded = tokenizer.encode_plus(\n","            \" \".join(token_list),\n","            None,\n","            add_special_tokens=True,\n","            max_length=max_length,\n","            truncation=True,\n","            padding=\"max_length\",\n","        )\n","\n","        ids, mask = encoded[\"input_ids\"], encoded[\"attention_mask\"]\n","\n","        bpe_head_mask = [0]\n","        bpe_tail_mask = [0]\n","        head_ids = [-100]\n","        dep_ids = [-100]\n","        pos_ids = [-100]  # --> CLS token\n","\n","        for token, head, dep, pos in zip(token_list, head_list, dep_list, pos_list):\n","            bpe_len = len(tokenizer.tokenize(token))\n","            head_token_mask = [1] + [0] * (bpe_len - 1)\n","            tail_token_mask = [0] * (bpe_len - 1) + [1]\n","            bpe_head_mask.extend(head_token_mask)\n","            bpe_tail_mask.extend(tail_token_mask)\n","\n","            head_mask = [head] + [-100] * (bpe_len - 1)\n","            head_ids.extend(head_mask)\n","            dep_mask = [dep_label_map[dep]] + [-100] * (bpe_len - 1)\n","            dep_ids.extend(dep_mask)\n","            pos_mask = [pos_label_map[pos]] + [-100] * (bpe_len - 1)\n","            pos_ids.extend(pos_mask)\n","\n","        bpe_head_mask.append(0)\n","        bpe_tail_mask.append(0)\n","        head_ids.append(-100)\n","        dep_ids.append(-100)  # END token\n","        bpe_head_mask.extend([0] * (max_length - len(bpe_head_mask)))  # padding by max_len\n","        bpe_tail_mask.extend([0] * (max_length - len(bpe_tail_mask)))  # padding by max_len\n","        head_ids.extend([-100] * (max_length - len(head_ids)))  # padding by max_len\n","        dep_ids.extend([-100] * (max_length - len(dep_ids)))  # padding by max_len\n","        pos_ids.extend([-100] * (max_length - len(pos_ids)))\n","\n","        feature = KlueDPInputFeatures(\n","            guid=example.guid,\n","            ids=ids,\n","            mask=mask,\n","            bpe_head_mask=bpe_head_mask,\n","            bpe_tail_mask=bpe_tail_mask,\n","            head_ids=head_ids,\n","            dep_ids=dep_ids,\n","            pos_ids=pos_ids,\n","        )\n","        features.append(feature)\n","\n","        for feature in features[:3]:\n","            logger.info(\"*** Example ***\")\n","            logger.info(\"input_ids: %s\" % feature.input_ids)\n","            logger.info(\"attention_mask: %s\" % feature.attention_mask)\n","            logger.info(\"bpe_head_mask: %s\" % feature.bpe_head_mask)\n","            logger.info(\"bpe_tail_mask: %s\" % feature.bpe_tail_mask)\n","            logger.info(\"head_id: %s\" % feature.head_ids)\n","            logger.info(\"dep_ids: %s\" % feature.dep_ids)\n","            logger.info(\"pos_ids: %s\" % feature.pos_ids)\n","\n","        return features\n","\n","    def _convert_features(self, examples: List[KlueDPInputExample]) -> List[KlueDPInputFeatures]:\n","        return self.convert_examples_to_features(\n","            examples,\n","            self.tokenizer,\n","            max_length=self.max_seq_length,\n","            dep_label_list=get_dep_labels(),\n","            pos_label_list=get_pos_labels(),\n","        )\n","\n","    def _create_dataset(self, file_path: str, dataset_type: str) -> TensorDataset:\n","        examples = self._create_examples(file_path, dataset_type)\n","        features = self._convert_features(examples)\n","\n","        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","        all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n","        all_bpe_head_mask = torch.tensor([f.bpe_head_mask for f in features], dtype=torch.long)\n","        all_bpe_tail_mask = torch.tensor([f.bpe_tail_mask for f in features], dtype=torch.long)\n","        all_head_ids = torch.tensor([f.head_ids for f in features], dtype=torch.long)\n","        all_dep_ids = torch.tensor([f.dep_ids for f in features], dtype=torch.long)\n","        all_pos_ids = torch.tensor([f.pos_ids for f in features], dtype=torch.long)\n","\n","        return TensorDataset(\n","            all_input_ids,\n","            all_attention_mask,\n","            all_bpe_head_mask,\n","            all_bpe_tail_mask,\n","            all_head_ids,\n","            all_dep_ids,\n","            all_pos_ids,\n","        )\n","\n","    def collate_fn(self, batch: List[Tuple]) -> Tuple[torch.Tensor, Any, Any, Any]:\n","        # 1. set args\n","        batch_size = len(batch)\n","        pos_padding_idx = None if self.hparams.no_pos else len(get_pos_labels())\n","        # 2. build inputs : input_ids, attention_mask, bpe_head_mask, bpe_tail_mask\n","        batch_input_ids = []\n","        batch_attention_masks = []\n","        batch_bpe_head_masks = []\n","        batch_bpe_tail_masks = []\n","        for batch_id in range(batch_size):\n","            (\n","                input_id,\n","                attention_mask,\n","                bpe_head_mask,\n","                bpe_tail_mask,\n","                _,\n","                _,\n","                _,\n","            ) = batch[batch_id]\n","            batch_input_ids.append(input_id)\n","            batch_attention_masks.append(attention_mask)\n","            batch_bpe_head_masks.append(bpe_head_mask)\n","            batch_bpe_tail_masks.append(bpe_tail_mask)\n","        # 2. build inputs : packing tensors\n","        # 나는 밥을 먹는다. => [CLS] 나 ##는 밥 ##을 먹 ##는 ##다 . [SEP]\n","        # input_id : [2, 717, 2259, 1127, 2069, 1059, 2259, 2062, 18, 3, 0, 0, ...]\n","        # bpe_head_mask : [0, 1, 0, 1, 0, 1, 0, 0, 0, 0, ...] (indicate word start (head) idx)\n","        input_ids = torch.stack(batch_input_ids)\n","        attention_masks = torch.stack(batch_attention_masks)\n","        bpe_head_masks = torch.stack(batch_bpe_head_masks)\n","        bpe_tail_masks = torch.stack(batch_bpe_tail_masks)\n","        # 3. token_to_words : set in-batch max_word_length\n","        max_word_length = max(torch.sum(bpe_head_masks, dim=1)).item()\n","        # 3. token_to_words : placeholders\n","        head_ids = torch.zeros(batch_size, max_word_length).long()\n","        type_ids = torch.zeros(batch_size, max_word_length).long()\n","        pos_ids = torch.zeros(batch_size, max_word_length + 1).long()\n","        mask_e = torch.zeros(batch_size, max_word_length + 1).long()\n","        # 3. token_to_words : head_ids, type_ids, pos_ids, mask_e, mask_d\n","        for batch_id in range(batch_size):\n","            (\n","                _,\n","                _,\n","                bpe_head_mask,\n","                _,\n","                token_head_ids,\n","                token_type_ids,\n","                token_pos_ids,\n","            ) = batch[batch_id]\n","            # head_id : [1, 3, 5] (prediction candidates)\n","            # token_head_ids : [-1, 3, -1, 3, -1, 0, -1, -1, -1, .-1, ...] (ground truth head ids)\n","            head_id = [i for i, token in enumerate(bpe_head_mask) if token == 1]\n","            word_length = len(head_id)\n","            head_id.extend([0] * (max_word_length - word_length))\n","            head_ids[batch_id] = token_head_ids[head_id]\n","            type_ids[batch_id] = token_type_ids[head_id]\n","\n","            pos_ids[batch_id][0] = torch.tensor(pos_padding_idx)\n","            pos_ids[batch_id][1:] = token_pos_ids[head_id]\n","            pos_ids[batch_id][int(torch.sum(bpe_head_mask)) + 1 :] = torch.tensor(pos_padding_idx)\n","            mask_e[batch_id] = torch.LongTensor([1] * (word_length + 1) + [0] * (max_word_length - word_length))\n","        mask_d = mask_e[:, 1:]\n","        # 4. pack everything\n","        masks = (attention_masks, bpe_head_masks, bpe_tail_masks, mask_e, mask_d)\n","        ids = (head_ids, type_ids, pos_ids)\n","\n","        return input_ids, masks, ids, max_word_length\n"],"metadata":{"id":"7soDhb9X1QPf","executionInfo":{"status":"ok","timestamp":1679737875609,"user_tz":-540,"elapsed":3,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}}},"id":"7soDhb9X1QPf","execution_count":12,"outputs":[]},{"cell_type":"code","source":["dataprocessor = KlueDPProcessor(128, tokenizer)\n","\n","\n","\n","\n"],"metadata":{"id":"oyPBtd99z9n9","executionInfo":{"status":"ok","timestamp":1679737924810,"user_tz":-540,"elapsed":353,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}}},"id":"oyPBtd99z9n9","execution_count":17,"outputs":[]},{"cell_type":"code","source":["train_dataset = dataprocessor._create_dataset('/content/klue-dp-v1.1_train.tsv','train')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TZwV7jp-GyM0","executionInfo":{"status":"ok","timestamp":1679737938608,"user_tz":-540,"elapsed":10827,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}},"outputId":"a3cedb61-924f-4e02-ac76-1967a55fe4e9"},"id":"TZwV7jp-GyM0","execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:*** Example ***\n","INFO:root:input_ids: [2, 4162, 4238, 2069, 1160, 2460, 14834, 6717, 7285, 6664, 27562, 6539, 25286, 2079, 6711, 15351, 8673, 2151, 4895, 16, 10879, 1283, 3781, 2069, 8631, 17807, 2371, 2062, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:bpe_head_mask: [0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:bpe_tail_mask: [0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:head_id: [-100, 2, 3, -100, 14, -100, 5, 14, -100, 7, -100, 10, -100, -100, 10, -100, 10, -100, -100, -100, 11, 12, 14, -100, 14, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","INFO:root:dep_ids: [-100, 0, 5, -100, 2, -100, 0, 3, -100, 0, -100, 8, -100, -100, 7, -100, 7, -100, -100, -100, 0, 0, 5, -100, 6, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","INFO:root:pos_ids: [-100, 0, 20, -100, 27, -100, 1, 17, -100, 1, -100, 19, -100, -100, 15, -100, 36, -100, -100, -100, 0, 0, 20, -100, 13, 35, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","INFO:root:*** Example ***\n","INFO:root:input_ids: [2, 3814, 11755, 2170, 2259, 605, 2043, 2347, 1370, 16, 648, 2163, 1370, 16, 26859, 2626, 617, 16, 1526, 2137, 2383, 1370, 16, 7503, 11, 27884, 2059, 2343, 11, 16, 1510, 2119, 2551, 1370, 2116, 5419, 2371, 2062, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:bpe_head_mask: [0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:bpe_tail_mask: [0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:head_id: [-100, 2, 15, -100, -100, 4, -100, -100, 14, -100, 6, -100, 14, -100, 8, -100, 14, -100, 10, -100, -100, 14, -100, 12, 14, -100, -100, -100, -100, -100, 14, -100, -100, 15, -100, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","INFO:root:dep_ids: [-100, 0, 1, -100, -100, 0, -100, -100, 7, -100, 0, -100, 7, -100, 0, -100, 7, -100, 0, -100, -100, 7, -100, 0, 7, -100, -100, -100, -100, -100, 0, -100, -100, 3, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","INFO:root:pos_ids: [-100, 0, 24, -100, -100, 1, -100, -100, 36, -100, 1, -100, 36, -100, 1, -100, 36, -100, 1, -100, -100, 36, -100, 0, 36, -100, -100, -100, -100, -100, 1, -100, -100, 17, -100, 35, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","INFO:root:*** Example ***\n","INFO:root:input_ids: [2, 3744, 3616, 2259, 4222, 2200, 3797, 10121, 2130, 4424, 13523, 2079, 7061, 7285, 4922, 2052, 5451, 2119, 10965, 2031, 2170, 2318, 5456, 2200, 723, 2259, 4000, 9407, 2069, 1410, 2359, 2219, 3606, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:bpe_head_mask: [0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:bpe_tail_mask: [0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:head_id: [-100, 14, 14, -100, 4, -100, -100, 6, -100, 6, 7, -100, 12, -100, 9, -100, 12, -100, 12, -100, -100, -100, 12, -100, 13, -100, -100, 14, -100, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","INFO:root:dep_ids: [-100, 1, 3, -100, 1, -100, -100, 4, -100, 0, 8, -100, 3, -100, 3, -100, 2, -100, 1, -100, -100, -100, 1, -100, 4, -100, -100, 5, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","INFO:root:pos_ids: [-100, 0, 24, -100, 21, -100, -100, 29, -100, 0, 19, -100, 17, -100, 17, -100, 27, -100, 21, -100, -100, -100, 21, -100, 29, -100, -100, 20, -100, 35, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"]}]},{"cell_type":"code","source":["validation_dataset = dataprocessor._create_dataset('/content/klue-dp-v1.1_dev.tsv','validation')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pf0tFXyeHU3P","executionInfo":{"status":"ok","timestamp":1679737940378,"user_tz":-540,"elapsed":1773,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}},"outputId":"7d47692d-3a54-4ccb-e324-4bc42b203a2d"},"id":"pf0tFXyeHU3P","execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:*** Example ***\n","INFO:root:input_ids: [2, 11, 47, 3360, 4889, 2195, 115, 13668, 2142, 2052, 27333, 2364, 2079, 8209, 2170, 4998, 2069, 12823, 2062, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:bpe_head_mask: [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:bpe_tail_mask: [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:head_id: [-100, 2, -100, -100, -100, -100, -100, 6, -100, -100, 4, -100, -100, 6, -100, 6, -100, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","INFO:root:dep_ids: [-100, 0, -100, -100, -100, -100, -100, 3, -100, -100, 8, -100, -100, 1, -100, 5, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","INFO:root:pos_ids: [-100, 37, -100, -100, -100, -100, -100, 17, -100, -100, 19, -100, -100, 21, -100, 20, -100, 35, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","INFO:root:*** Example ***\n","INFO:root:input_ids: [2, 7443, 2259, 4153, 2079, 4646, 2470, 4103, 2170, 3618, 4901, 2116, 28959, 8248, 1670, 2483, 2522, 648, 2483, 2116, 4006, 2125, 3992, 2138, 1170, 4000, 1540, 2069, 4089, 2371, 2062, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:bpe_head_mask: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:bpe_tail_mask: [0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:head_id: [-100, 15, -100, 4, -100, 4, -100, 5, -100, 6, 7, -100, 8, 13, 10, -100, -100, 13, -100, -100, 12, -100, 13, -100, 14, -100, 15, -100, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","INFO:root:dep_ids: [-100, 3, -100, 8, -100, 4, -100, 1, -100, 4, 3, -100, 2, 2, 7, -100, -100, 3, -100, -100, 10, -100, 5, -100, 4, -100, 5, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","INFO:root:pos_ids: [-100, 24, -100, 19, -100, 29, -100, 21, -100, 29, 17, -100, 27, 27, 15, -100, -100, 17, -100, -100, 31, -100, 20, -100, 29, -100, 20, -100, 35, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","INFO:root:*** Example ***\n","INFO:root:input_ids: [2, 3651, 2259, 3834, 2178, 2062, 5947, 7041, 4054, 27135, 4159, 2138, 1122, 2414, 3706, 2052, 2359, 2062, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:bpe_head_mask: [0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:bpe_tail_mask: [0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","INFO:root:head_id: [-100, 4, -100, 4, -100, -100, 4, 7, 7, -100, 7, -100, 8, -100, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","INFO:root:dep_ids: [-100, 3, -100, 1, -100, -100, 6, 2, 1, -100, 5, -100, 4, -100, 9, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","INFO:root:pos_ids: [-100, 24, -100, 21, -100, -100, 13, 27, 21, -100, 20, -100, 29, -100, 35, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"]}]},{"cell_type":"code","source":["\n","class DPTransformer(BaseTransformer):\n","\n","    mode = Mode.DependencyParsing\n","\n","    def __init__(self, hparams: Union[Dict[str, Any], argparse.Namespace], metrics: dict = {}) -> None:\n","        if type(hparams) == dict:\n","            hparams = argparse.Namespace(**hparams)\n","\n","        super().__init__(\n","            hparams,\n","            num_labels=None,\n","            mode=self.mode,\n","            model_type=AutoModel,\n","            metrics=metrics,\n","        )\n","\n","        self.hidden_size = hparams.hidden_size\n","        self.input_size = self.model.config.hidden_size\n","        self.arc_space = hparams.arc_space\n","        self.type_space = hparams.type_space\n","\n","        self.n_pos_labels = len(get_pos_labels())\n","        self.n_dp_labels = len(get_dep_labels())\n","\n","        if hparams.no_pos:\n","            self.pos_embedding = None\n","        else:\n","            self.pos_embedding = nn.Embedding(self.n_pos_labels + 1, hparams.pos_dim)\n","\n","        enc_dim = self.input_size * 2\n","        if self.pos_embedding is not None:\n","            enc_dim += hparams.pos_dim\n","\n","        self.encoder = nn.LSTM(\n","            enc_dim,\n","            self.hidden_size,\n","            hparams.encoder_layers,\n","            batch_first=True,\n","            dropout=0.33,\n","            bidirectional=True,\n","        )\n","        self.decoder = nn.LSTM(\n","            self.hidden_size, self.hidden_size, hparams.decoder_layers, batch_first=True, dropout=0.33\n","        )\n","\n","        self.dropout = nn.Dropout2d(p=0.33)\n","\n","        self.src_dense = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        self.hx_dense = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","\n","        self.arc_c = nn.Linear(self.hidden_size * 2, self.arc_space)\n","        self.type_c = nn.Linear(self.hidden_size * 2, self.type_space)\n","        self.arc_h = nn.Linear(self.hidden_size, self.arc_space)\n","        self.type_h = nn.Linear(self.hidden_size, self.type_space)\n","\n","        self.attention = BiAttention(self.arc_space, self.arc_space, 1)\n","        self.bilinear = BiLinear(self.type_space, self.type_space, self.n_dp_labels)\n","\n","    @overrides\n","    def forward(\n","        self,\n","        bpe_head_mask: torch.Tensor,\n","        bpe_tail_mask: torch.Tensor,\n","        pos_ids: torch.Tensor,\n","        head_ids: torch.Tensor,\n","        max_word_length: int,\n","        mask_e: torch.Tensor,\n","        mask_d: torch.Tensor,\n","        batch_index: torch.Tensor,\n","        is_training: bool = True,\n","        **inputs: torch.Tensor,\n","    ) -> Tuple[torch.Tensor, torch.Tensor]:\n","\n","        outputs = self.model(**inputs)\n","        outputs = outputs[0]\n","        outputs, sent_len = self.resize_outputs(outputs, bpe_head_mask, bpe_tail_mask, max_word_length)\n","\n","        if self.pos_embedding is not None:\n","            pos_outputs = self.pos_embedding(pos_ids)\n","            pos_outputs = self.dropout(pos_outputs)\n","            outputs = torch.cat([outputs, pos_outputs], dim=2)\n","\n","        # encoder\n","        packed_outputs = pack_padded_sequence(outputs, sent_len, batch_first=True, enforce_sorted=False)\n","        encoder_outputs, hn = self.encoder(packed_outputs)\n","        encoder_outputs, outputs_len = pad_packed_sequence(encoder_outputs, batch_first=True)\n","        encoder_outputs = self.dropout(encoder_outputs.transpose(1, 2)).transpose(1, 2)  # apply dropout for last layer\n","        hn = self._transform_decoder_init_state(hn)\n","\n","        # decoder\n","        src_encoding = F.elu(self.src_dense(encoder_outputs[:, 1:]))\n","        sent_len = [i - 1 for i in sent_len]\n","        packed_outputs = pack_padded_sequence(src_encoding, sent_len, batch_first=True, enforce_sorted=False)\n","        decoder_outputs, _ = self.decoder(packed_outputs, hn)\n","        decoder_outputs, outputs_len = pad_packed_sequence(decoder_outputs, batch_first=True)\n","        decoder_outputs = self.dropout(decoder_outputs.transpose(1, 2)).transpose(1, 2)  # apply dropout for last layer\n","\n","        # compute output for arc and type\n","        arc_c = F.elu(self.arc_c(encoder_outputs))\n","        type_c = F.elu(self.type_c(encoder_outputs))\n","\n","        arc_h = F.elu(self.arc_h(decoder_outputs))\n","        type_h = F.elu(self.type_h(decoder_outputs))\n","\n","        out_arc = self.attention(arc_h, arc_c, mask_d=mask_d, mask_e=mask_e).squeeze(dim=1)\n","\n","        # use predicted head_ids when validation step\n","        if not is_training:\n","            head_ids = torch.argmax(out_arc, dim=2)\n","\n","        type_c = type_c[batch_index, head_ids.data.t()].transpose(0, 1).contiguous()\n","        out_type = self.bilinear(type_h, type_c)\n","\n","        return out_arc, out_type\n","\n","    @overrides\n","    def training_step(self, batch: List[torch.Tensor], batch_idx: int) -> dict:\n","        input_ids, masks, ids, max_word_length = batch\n","        attention_mask, bpe_head_mask, bpe_tail_mask, mask_e, mask_d = masks\n","        head_ids, type_ids, pos_ids = ids\n","        inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n","\n","        batch_size = head_ids.size()[0]\n","        batch_index = torch.arange(0, int(batch_size)).long()\n","        head_index = (\n","            torch.arange(0, max_word_length).view(max_word_length, 1).expand(max_word_length, batch_size).long()\n","        )\n","\n","        # forward\n","        out_arc, out_type = self(\n","            bpe_head_mask, bpe_tail_mask, pos_ids, head_ids, max_word_length, mask_e, mask_d, batch_index, **inputs\n","        )\n","\n","        # compute loss\n","        minus_inf = -1e8\n","        minus_mask_d = (1 - mask_d) * minus_inf\n","        minus_mask_e = (1 - mask_e) * minus_inf\n","        out_arc = out_arc + minus_mask_d.unsqueeze(2) + minus_mask_e.unsqueeze(1)\n","\n","        loss_arc = F.log_softmax(out_arc, dim=2)\n","        loss_type = F.log_softmax(out_type, dim=2)\n","\n","        loss_arc = loss_arc * mask_d.unsqueeze(2) * mask_e.unsqueeze(1)\n","        loss_type = loss_type * mask_d.unsqueeze(2)\n","        num = mask_d.sum()\n","\n","        loss_arc = loss_arc[batch_index, head_index, head_ids.data.t()].transpose(0, 1)\n","        loss_type = loss_type[batch_index, head_index, type_ids.data.t()].transpose(0, 1)\n","        loss_arc = -loss_arc.sum() / num\n","        loss_type = -loss_type.sum() / num\n","        loss = loss_arc + loss_type\n","\n","        self.log(\"train/loss_arc\", loss_arc)\n","        self.log(\"train/loss_type\", loss_type)\n","        self.log(\"train/loss\", loss)\n","\n","        return {\"loss\": loss}\n","\n","    @overrides\n","    def validation_step(self, batch: List[torch.Tensor], batch_idx: int, data_type: str = \"valid\") -> dict:\n","        input_ids, masks, ids, max_word_length = batch\n","        attention_mask, bpe_head_mask, bpe_tail_mask, mask_e, mask_d = masks\n","        head_ids, type_ids, pos_ids = ids\n","        inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n","\n","        batch_index = torch.arange(0, head_ids.size()[0]).long()\n","\n","        out_arc, out_type = self(\n","            bpe_head_mask,\n","            bpe_tail_mask,\n","            pos_ids,\n","            head_ids,\n","            max_word_length,\n","            mask_e,\n","            mask_d,\n","            batch_index,\n","            is_training=False,\n","            **inputs,\n","        )\n","\n","        # predict arc and its type\n","        heads = torch.argmax(out_arc, dim=2)\n","        types = torch.argmax(out_type, dim=2)\n","\n","        preds = DPResult(heads, types)\n","        labels = DPResult(head_ids, type_ids)\n","\n","        return {\"preds\": preds, \"labels\": labels}\n","\n","    @overrides\n","    def validation_epoch_end(\n","        self, outputs: List[Dict[str, DPResult]], data_type: str = \"valid\", write_predictions: bool = False\n","    ) -> None:\n","        all_preds = []\n","        all_labels = []\n","        for output in zip(outputs):\n","            all_preds.append(output[0][\"preds\"])\n","            all_labels.append(output[0][\"labels\"])\n","\n","        if write_predictions is True:\n","            self.write_prediction_file(all_preds, all_labels)\n","\n","        self._set_metrics_device()\n","        for k, metric in self.metrics.items():\n","            metric(all_preds, all_labels)\n","            self.log(f\"{data_type}/{k}\", metric, on_step=False, on_epoch=True, logger=True)\n","\n","    def write_prediction_file(self, prs: List[DPResult], gts: List[DPResult]) -> None:\n","        \"\"\"Write head, head type predictions and corresponding labels to json file. Each line indicates a word.\"\"\"\n","        head_preds, type_preds, head_labels, type_labels = self._flatten_prediction_and_labels(prs, gts)\n","        save_path = self.output_dir.joinpath(\"transformers/pred\")\n","        if not os.path.exists(save_path):\n","            os.makedirs(save_path, exist_ok=True)\n","        with open(os.path.join(save_path, f\"pred-{self.step_count}.json\"), \"w\", encoding=\"utf-8\") as f:\n","            for h, t, hl, tl in zip(head_preds, type_preds, head_labels, type_labels):\n","                f.write(\" \".join([str(h), str(t), str(hl), str(tl)]) + \"\\n\")\n","\n","    def _flatten_prediction_and_labels(\n","        self, preds: List[DPResult], labels: List[DPResult]\n","    ) -> Tuple[List, List, List, List]:\n","        \"\"\"Convert prediction and labels to np.array and remove -1s.\"\"\"\n","        head_pred_list = list()\n","        head_label_list = list()\n","        type_pred_list = list()\n","        type_label_list = list()\n","        for pred, label in zip(preds, labels):\n","            head_pred_list += pred.heads.cpu().flatten().tolist()\n","            head_label_list += label.heads.cpu().flatten().tolist()\n","            type_pred_list += pred.types.cpu().flatten().tolist()\n","            type_label_list += label.types.cpu().flatten().tolist()\n","        head_preds = np.array(head_pred_list)\n","        head_labels = np.array(head_label_list)\n","        type_preds = np.array(type_pred_list)\n","        type_labels = np.array(type_label_list)\n","\n","        index = [i for i, label in enumerate(head_labels) if label == -1]\n","        head_preds = np.delete(head_preds, index)\n","        head_labels = np.delete(head_labels, index)\n","        index = [i for i, label in enumerate(type_labels) if label == -1]\n","        type_preds = np.delete(type_preds, index)\n","        type_labels = np.delete(type_labels, index)\n","\n","        return (\n","            head_preds.tolist(),\n","            type_preds.tolist(),\n","            head_labels.tolist(),\n","            type_labels.tolist(),\n","        )\n","\n","    @pl.utilities.rank_zero_only\n","    def on_save_checkpoint(self, checkpoint: Dict[str, Any]) -> None:\n","        save_path = self.output_dir.joinpath(\"transformers\")\n","        if not os.path.exists(save_path):\n","            os.makedirs(save_path, exist_ok=True)\n","        self.config.save_step = self.step_count\n","        torch.save(self.state_dict(), save_path.joinpath(\"dp-model.bin\"))\n","        self.config.save_pretrained(save_path)\n","        self.tokenizer.save_pretrained(save_path)\n","\n","    @staticmethod\n","    def add_specific_args(parser: argparse.ArgumentParser, root_dir: str) -> argparse.ArgumentParser:\n","        BaseTransformer.add_specific_args(parser, root_dir)\n","        parser.add_argument(\"--encoder_layers\", default=1, type=int, help=\"Number of layers of encoder\")\n","        parser.add_argument(\"--decoder_layers\", default=1, type=int, help=\"Number of layers of decoder\")\n","        parser.add_argument(\"--hidden_size\", default=768, type=int, help=\"Number of hidden units in LSTM\")\n","        parser.add_argument(\"--arc_space\", default=512, type=int, help=\"Dimension of tag space\")\n","        parser.add_argument(\"--type_space\", default=256, type=int, help=\"Dimension of tag space\")\n","        parser.add_argument(\"--no_pos\", action=\"store_true\", help=\"Do not use pos feature in head layers\")\n","        parser.add_argument(\"--pos_dim\", default=256, type=int, help=\"Dimension of pos embedding\")\n","        args = parser.parse_args()\n","        if not args.no_pos and args.pos_dim <= 0:\n","            parser.error(\"--pos_dim should be a positive integer when --no_pos is False.\")\n","        return parser\n","\n","    def resize_outputs(\n","        self, outputs: torch.Tensor, bpe_head_mask: torch.Tensor, bpe_tail_mask: torch.Tensor, max_word_length: int\n","    ) -> Tuple[torch.Tensor, List]:\n","        \"\"\"Resize output of pre-trained transformers (bsz, max_token_length, hidden_dim) to word-level outputs (bsz, max_word_length, hidden_dim*2). \"\"\"\n","        batch_size, input_size, hidden_size = outputs.size()\n","        word_outputs = torch.zeros(batch_size, max_word_length + 1, hidden_size * 2).to(outputs.device)\n","        sent_len = list()\n","\n","        for batch_id in range(batch_size):\n","            head_ids = [i for i, token in enumerate(bpe_head_mask[batch_id]) if token == 1]\n","            tail_ids = [i for i, token in enumerate(bpe_tail_mask[batch_id]) if token == 1]\n","            assert len(head_ids) == len(tail_ids)\n","\n","            word_outputs[batch_id][0] = torch.cat(\n","                (outputs[batch_id][0], outputs[batch_id][0])\n","            )  # replace root with [CLS]\n","            for i, (head, tail) in enumerate(zip(head_ids, tail_ids)):\n","                word_outputs[batch_id][i + 1] = torch.cat((outputs[batch_id][head], outputs[batch_id][tail]))\n","            sent_len.append(i + 2)\n","\n","        return word_outputs, sent_len\n","\n","    def _transform_decoder_init_state(self, hn: torch.Tensor) -> torch.Tensor:\n","        hn, cn = hn\n","        cn = cn[-2:]  # take the last layer\n","        _, batch_size, hidden_size = cn.size()\n","        cn = cn.transpose(0, 1).contiguous()\n","        cn = cn.view(batch_size, 1, 2 * hidden_size).transpose(0, 1)\n","        cn = self.hx_dense(cn)\n","        if self.decoder.num_layers > 1:\n","            cn = torch.cat(\n","                [\n","                    cn,\n","                    torch.autograd.Variable(cn.data.new(self.decoder.num_layers - 1, batch_size, hidden_size).zero_()),\n","                ],\n","                dim=0,\n","            )\n","        hn = torch.tanh(cn)\n","        hn = (hn, cn)\n","        return hn\n","\n","\n","class BiAttention(nn.Module):\n","    def __init__(  # type: ignore[no-untyped-def]\n","        self, input_size_encoder: int, input_size_decoder: int, num_labels: int, biaffine: bool = True, **kwargs\n","    ) -> None:\n","        super(BiAttention, self).__init__()\n","        self.input_size_encoder = input_size_encoder\n","        self.input_size_decoder = input_size_decoder\n","        self.num_labels = num_labels\n","        self.biaffine = biaffine\n","\n","        self.W_e = Parameter(torch.Tensor(self.num_labels, self.input_size_encoder))\n","        self.W_d = Parameter(torch.Tensor(self.num_labels, self.input_size_decoder))\n","        self.b = Parameter(torch.Tensor(self.num_labels, 1, 1))\n","        if self.biaffine:\n","            self.U = Parameter(torch.Tensor(self.num_labels, self.input_size_decoder, self.input_size_encoder))\n","        else:\n","            self.register_parameter(\"U\", None)\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self) -> None:\n","        nn.init.xavier_uniform_(self.W_e)\n","        nn.init.xavier_uniform_(self.W_d)\n","        nn.init.constant_(self.b, 0.0)\n","        if self.biaffine:\n","            nn.init.xavier_uniform_(self.U)\n","\n","    def forward(\n","        self,\n","        input_d: torch.Tensor,\n","        input_e: torch.Tensor,\n","        mask_d: Optional[torch.Tensor] = None,\n","        mask_e: Optional[torch.Tensor] = None,\n","    ) -> torch.Tensor:\n","        assert input_d.size(0) == input_e.size(0)\n","        batch, length_decoder, _ = input_d.size()\n","        _, length_encoder, _ = input_e.size()\n","\n","        out_d = torch.matmul(self.W_d, input_d.transpose(1, 2)).unsqueeze(3)\n","        out_e = torch.matmul(self.W_e, input_e.transpose(1, 2)).unsqueeze(2)\n","\n","        if self.biaffine:\n","            output = torch.matmul(input_d.unsqueeze(1), self.U)\n","            output = torch.matmul(output, input_e.unsqueeze(1).transpose(2, 3))\n","            output = output + out_d + out_e + self.b\n","        else:\n","            output = out_d + out_d + self.b\n","\n","        if mask_d is not None:\n","            output = output * mask_d.unsqueeze(1).unsqueeze(3) * mask_e.unsqueeze(1).unsqueeze(2)\n","\n","        return output\n","\n","\n","class BiLinear(nn.Module):\n","    def __init__(self, left_features: int, right_features: int, out_features: int):\n","        super(BiLinear, self).__init__()\n","        self.left_features = left_features\n","        self.right_features = right_features\n","        self.out_features = out_features\n","\n","        self.U = Parameter(torch.Tensor(self.out_features, self.left_features, self.right_features))\n","        self.W_l = Parameter(torch.Tensor(self.out_features, self.left_features))\n","        self.W_r = Parameter(torch.Tensor(self.out_features, self.left_features))\n","        self.bias = Parameter(torch.Tensor(out_features))\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self) -> None:\n","        nn.init.xavier_uniform_(self.W_l)\n","        nn.init.xavier_uniform_(self.W_r)\n","        nn.init.constant_(self.bias, 0.0)\n","        nn.init.xavier_uniform_(self.U)\n","\n","    def forward(self, input_left: torch.Tensor, input_right: torch.Tensor) -> torch.Tensor:\n","        left_size = input_left.size()\n","        right_size = input_right.size()\n","        assert left_size[:-1] == right_size[:-1], \"batch size of left and right inputs mis-match: (%s, %s)\" % (\n","            left_size[:-1],\n","            right_size[:-1],\n","        )\n","        batch = int(np.prod(left_size[:-1]))\n","\n","        input_left = input_left.contiguous().view(batch, self.left_features)\n","        input_right = input_right.contiguous().view(batch, self.right_features)\n","\n","        output = F.bilinear(input_left, input_right, self.U, self.bias)\n","        output = output + F.linear(input_left, self.W_l, None) + F.linear(input_right, self.W_r, None)\n","        return output.view(left_size[:-1] + (self.out_features,))\n"],"metadata":{"id":"dUyQv7ciz-If"},"id":"dUyQv7ciz-If","execution_count":null,"outputs":[]},{"cell_type":"code","source":["P"],"metadata":{"id":"MiMqkiBN6bmw"},"id":"MiMqkiBN6bmw","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ua6TjyvFBBgB"},"id":"Ua6TjyvFBBgB","execution_count":null,"outputs":[]},{"cell_type":"code","source":["pos_labels = get_pos_labels()\n","dep_labels = get_dep_labels()"],"metadata":{"id":"hA5leHODBBwG"},"id":"hA5leHODBBwG","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"a9gZYv4uzwLq"},"id":"a9gZYv4uzwLq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/KLUE-benchmark/KLUE-baseline.git"],"metadata":{"id":"bqBYF2Se9HbJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679674194249,"user_tz":-540,"elapsed":2336,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}},"outputId":"12480caa-d46b-43ea-a253-f55888bc8bca"},"id":"bqBYF2Se9HbJ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'KLUE-baseline'...\n","remote: Enumerating objects: 74, done.\u001b[K\n","remote: Counting objects: 100% (74/74), done.\u001b[K\n","remote: Compressing objects: 100% (64/64), done.\u001b[K\n","remote: Total 74 (delta 18), reused 63 (delta 9), pack-reused 0\u001b[K\n","Unpacking objects: 100% (74/74), 63.62 KiB | 651.00 KiB/s, done.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"gsXLaAuyXgHN"},"id":"gsXLaAuyXgHN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/KLUE-baseline"],"metadata":{"id":"2SixiUpXzweF","executionInfo":{"status":"ok","timestamp":1679674196498,"user_tz":-540,"elapsed":5,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f2710f9a-7e3e-449b-f91a-7e8cde16be6b"},"id":"2SixiUpXzweF","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/KLUE-baseline\n"]}]},{"cell_type":"code","source":["!pip install pytorch-lightning\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0tuZIlg4Xg05","executionInfo":{"status":"ok","timestamp":1679674283929,"user_tz":-540,"elapsed":9550,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}},"outputId":"74abc8de-ef3c-40c0-fc72-ccdb51358d8a"},"id":"0tuZIlg4Xg05","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.9/dist-packages (2.0.0)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (4.65.0)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (1.13.1+cu116)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (4.5.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (1.22.4)\n","Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (0.11.4)\n","Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (2023.3.0)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (6.0)\n","Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (23.0)\n","Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (0.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.27.1)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.12)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.2.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n"]}]},{"cell_type":"code","source":["!pip install overrides dataclasses"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":212},"id":"CK5ZHnO0Y9j-","executionInfo":{"status":"ok","timestamp":1679674401265,"user_tz":-540,"elapsed":9003,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}},"outputId":"805ee4dd-a8a7-4bfb-adef-6f3c2700b96b"},"id":"CK5ZHnO0Y9j-","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: overrides in /usr/local/lib/python3.9/dist-packages (7.3.1)\n","Collecting dataclasses\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Installing collected packages: dataclasses\n","Successfully installed dataclasses-0.6\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dataclasses"]}}},"metadata":{}}]},{"cell_type":"code","source":["\n","!sh ./run_all.sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MVv-ule4XEvJ","executionInfo":{"status":"ok","timestamp":1679674422048,"user_tz":-540,"elapsed":877,"user":{"displayName":"Dominic Lee","userId":"02703556362041959690"}},"outputId":"b2c015ac-e4ed-4f25-e9ea-69788faf5f6b"},"id":"MVv-ule4XEvJ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sh: 0: Can't open ./run_all.sh\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Cq9vkoyDXTvw"},"id":"Cq9vkoyDXTvw","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1GxGQxuGpuFDok5oSsIRPrfyUquLaUeS2","timestamp":1679206093962},{"file_id":"1SRitTkYGs6mDhay4-kPQNY0HJ5zEgMEd","timestamp":1679205024104},{"file_id":"15jiJJVSgykXGA8Hpndr_sOdOdlbF2B-A","timestamp":1679204697308},{"file_id":"15IVZv0LD1gDqICWvxwZte3A0-P27HQ8l","timestamp":1679191468837},{"file_id":"11qtug9b787c3hBBjK_ssieM6WD1ikUFv","timestamp":1679188331689},{"file_id":"https://github.com/Huffon/klue-transformers-tutorial/blob/master/natural_language_inference.ipynb","timestamp":1679058086182}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"95b7970d076b408dbf4f1c22ae112256":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9bc908af4a9c4376af8f8552a6f88c3c","IPY_MODEL_a09ca482422c49e4a873bf1e2effddd1","IPY_MODEL_9d0afd3d43494868b0f4818d8bd63463"],"layout":"IPY_MODEL_34d9dd10d0dc4c828b34405715680bd3"}},"9bc908af4a9c4376af8f8552a6f88c3c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16108b92b4574e88a8f5407563390269","placeholder":"​","style":"IPY_MODEL_c8f9556dd395494286beecae03d660a9","value":"100%"}},"a09ca482422c49e4a873bf1e2effddd1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_446d8b86da824c5c823041b48ed88096","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6d76b79a2bd44468a0e20adcd005fa58","value":2}},"9d0afd3d43494868b0f4818d8bd63463":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45ceaf6f3cb44103af376c5bb1c576d1","placeholder":"​","style":"IPY_MODEL_1c7ee48824cc41d1b3c8cf3f296da2c1","value":" 2/2 [00:00&lt;00:00, 50.62it/s]"}},"34d9dd10d0dc4c828b34405715680bd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16108b92b4574e88a8f5407563390269":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8f9556dd395494286beecae03d660a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"446d8b86da824c5c823041b48ed88096":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d76b79a2bd44468a0e20adcd005fa58":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"45ceaf6f3cb44103af376c5bb1c576d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c7ee48824cc41d1b3c8cf3f296da2c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a3feeb86d234d57a068ff7576544a62":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ad9f4a7c2e1e414e81e19f96f03f0cbf","IPY_MODEL_1034f18cc80b442890021f8eeb3ad1be","IPY_MODEL_874bffba26514b0a8dc855b1a14c19c0"],"layout":"IPY_MODEL_8874354c0bfd41eea1c7d0da1b5f01d8"}},"ad9f4a7c2e1e414e81e19f96f03f0cbf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e9acd67f87044d9aee23a4ed37f03b6","placeholder":"​","style":"IPY_MODEL_a8bc640b7d93476fb0ff53f7272b6057","value":"Downloading (…)okenizer_config.json: 100%"}},"1034f18cc80b442890021f8eeb3ad1be":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ad548dc5c5a4ef8b434fff1075e847a","max":289,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2a93346a50464032b0a4943e8df407b9","value":289}},"874bffba26514b0a8dc855b1a14c19c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30243c264c904dd58d6389d11635d75b","placeholder":"​","style":"IPY_MODEL_3c3e72678af64df09a29e3b131f3681e","value":" 289/289 [00:00&lt;00:00, 8.66kB/s]"}},"8874354c0bfd41eea1c7d0da1b5f01d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e9acd67f87044d9aee23a4ed37f03b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8bc640b7d93476fb0ff53f7272b6057":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ad548dc5c5a4ef8b434fff1075e847a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a93346a50464032b0a4943e8df407b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"30243c264c904dd58d6389d11635d75b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c3e72678af64df09a29e3b131f3681e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ef91e73cf8748cab38e6ebfd02145d3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_691efb7e903743a6839f9f26bccb37b9","IPY_MODEL_2a69d6aac70b45b88112a75012286fe7","IPY_MODEL_4121f786ee4a4084bb009587141147d4"],"layout":"IPY_MODEL_c70a307135374298ac167bc4948a6d91"}},"691efb7e903743a6839f9f26bccb37b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75d7a131e6ac40b7b03898441e66472b","placeholder":"​","style":"IPY_MODEL_bcfc09c414de496f82ac69d9f10e1490","value":"Downloading (…)lve/main/config.json: 100%"}},"2a69d6aac70b45b88112a75012286fe7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c12b63599bab4676a22b584801d1aae2","max":425,"min":0,"orientation":"horizontal","style":"IPY_MODEL_234ef66bd73b45e389eb37a85bf1dead","value":425}},"4121f786ee4a4084bb009587141147d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d26f0a0ce2c04f408657ac34da6d783e","placeholder":"​","style":"IPY_MODEL_816c42483e7e45dbb4ec944fa06661e9","value":" 425/425 [00:00&lt;00:00, 19.3kB/s]"}},"c70a307135374298ac167bc4948a6d91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75d7a131e6ac40b7b03898441e66472b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcfc09c414de496f82ac69d9f10e1490":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c12b63599bab4676a22b584801d1aae2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"234ef66bd73b45e389eb37a85bf1dead":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d26f0a0ce2c04f408657ac34da6d783e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"816c42483e7e45dbb4ec944fa06661e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52e3045105ca402da4ea94f522d769fd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_195395425ebe425bad365d5eec403fa6","IPY_MODEL_a9f7ad593e214072a54e8735fb2febf6","IPY_MODEL_20d393936ec843d3aa78e9168fe4a09f"],"layout":"IPY_MODEL_f0aee027338d44cbabb9ad9cd9b4898b"}},"195395425ebe425bad365d5eec403fa6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0eecb986aa654487922b6d982be7374a","placeholder":"​","style":"IPY_MODEL_3f5714a646434a8694acf2c023ed1c7f","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"a9f7ad593e214072a54e8735fb2febf6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c89f70b2de348c8b89d5e041f325a18","max":248477,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5ffc4d01b9a34d5a97378960226f189e","value":248477}},"20d393936ec843d3aa78e9168fe4a09f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fac3e74578764d96bcf93a0c6bf7439b","placeholder":"​","style":"IPY_MODEL_b84866ff9ee04c8abed8e9a8c9cd9887","value":" 248k/248k [00:00&lt;00:00, 919kB/s]"}},"f0aee027338d44cbabb9ad9cd9b4898b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0eecb986aa654487922b6d982be7374a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f5714a646434a8694acf2c023ed1c7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c89f70b2de348c8b89d5e041f325a18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ffc4d01b9a34d5a97378960226f189e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fac3e74578764d96bcf93a0c6bf7439b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b84866ff9ee04c8abed8e9a8c9cd9887":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a26a17c491d45a29b0118de8df55be9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e03183718003498c8e680bab85439104","IPY_MODEL_7c4f060286e1494987f700bef3b4e1f7","IPY_MODEL_ad3b3e441c854660b62324a5fb942c4c"],"layout":"IPY_MODEL_e21510ac88c54f27813ce1a1ee27a55d"}},"e03183718003498c8e680bab85439104":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_35aa963ce7fc460bae814e31a1cf0f21","placeholder":"​","style":"IPY_MODEL_9d24077ae03346918dfa4073d4688644","value":"Downloading (…)/main/tokenizer.json: 100%"}},"7c4f060286e1494987f700bef3b4e1f7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c200cfb8a2cf4beeb47be9f45b7739dc","max":494860,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93f149abba2545bd9c088fbe5242a5ea","value":494860}},"ad3b3e441c854660b62324a5fb942c4c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fd4ddd1e215467c90bb275802ee839e","placeholder":"​","style":"IPY_MODEL_b52e4ad67e01402ab3ddff1e9695eb36","value":" 495k/495k [00:00&lt;00:00, 1.39MB/s]"}},"e21510ac88c54f27813ce1a1ee27a55d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35aa963ce7fc460bae814e31a1cf0f21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d24077ae03346918dfa4073d4688644":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c200cfb8a2cf4beeb47be9f45b7739dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93f149abba2545bd9c088fbe5242a5ea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3fd4ddd1e215467c90bb275802ee839e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b52e4ad67e01402ab3ddff1e9695eb36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8899ccfab14f4d9fbf50a114a4a74fa6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a4de7f7528a54452a6680e9778aba182","IPY_MODEL_1d38920268f64136bd7c14f0668f04f9","IPY_MODEL_c5f93a66e9af43bd8c8286c121348c84"],"layout":"IPY_MODEL_95de76ea787b4607ba5168d2fa56261d"}},"a4de7f7528a54452a6680e9778aba182":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_56044980c60e41d6ac8e49e011f958ca","placeholder":"​","style":"IPY_MODEL_6bb78bc983d74713822792c91fed4c4e","value":"Downloading (…)cial_tokens_map.json: 100%"}},"1d38920268f64136bd7c14f0668f04f9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_06f8b07dc7f54659857041c44c20b069","max":125,"min":0,"orientation":"horizontal","style":"IPY_MODEL_25ad51c1a8c3438ca0e27ca31e34007d","value":125}},"c5f93a66e9af43bd8c8286c121348c84":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22029c9ab46c421981fa6c8ff78bf1d4","placeholder":"​","style":"IPY_MODEL_243b1784f29c43f6b4962f996c3950cb","value":" 125/125 [00:00&lt;00:00, 7.56kB/s]"}},"95de76ea787b4607ba5168d2fa56261d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56044980c60e41d6ac8e49e011f958ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bb78bc983d74713822792c91fed4c4e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06f8b07dc7f54659857041c44c20b069":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25ad51c1a8c3438ca0e27ca31e34007d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"22029c9ab46c421981fa6c8ff78bf1d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"243b1784f29c43f6b4962f996c3950cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}